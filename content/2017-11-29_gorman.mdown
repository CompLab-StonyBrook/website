Title: Frontiers Talk by Kyle Gorman
Author: CompLab
Date: 2017-11-29
Category: Events
Tags: invited speaker; NLP

<img style="width:20%; float:right" src="{filename}/images/2017-11-29_gorman.png" />

[Kyle Gorman](https://research.google.com/pubs/KyleGorman.html) will be giving a Frontiers talk in our department on November 29 at 4:00pm.
Kyle got his Ph.D. in Linguistics from Penn University in 2013 and is now working in Google's research group on natural language processing in New York.
One of the big splits in NLP is the divide between hand-crafted resources and automatically trained machine models.
In his talk, Kyle will contrast the usefulness of these two approaches for text normalization.

The title and abstract are below.

## Minimally supervised text normalization: algorithms and implementation 

Text normalization refers to methods used to convert text input from written (e.g., "$2.50", "5:15pm") to spoken form ("two dollars and fifty cents", "five fifteen PM"), primarily, though not exclusively, as part of a text-to-speech synthesis engine.
Unlike much of modern natural language processing, much of this process is currently accomplished using hand-written language-specific grammars rather than general-purpose machine learning engines, hindering our ability to scale.
In the first half of the talk, I describe experiments on learning text normalization for cardinal and ordinal numbers (Gorman & Sproat 2016), which accounts for much of the complexity of these language-specific grammars.
The first model I describe uses an end-to-end recurrent neural network.
The second model, drawing inspiration from the linguistics literature, uses finite-state transducers constructed with a minimal amount of training data. 
While both models achieve near-perfect performance, the latter model can be trained using several orders of magnitude less data than the former, making it particularly useful for low-resource languages.
I also describe some extensions of this work to other classes such as currency and time expressions.
The second half of the talk will discuss Pynini (Gorman 2016), an open-source Python library for grammar compilation, which is used to implement minimally supervised text normalization grammar induction.
I will review the design of this library and provide several tutorial examples drawn from morphophonology.

(This talk draws from work done in collaboration with [Richard Sproat](https://research.google.com/pubs/RichardSproat.html).)
